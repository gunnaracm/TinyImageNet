Правильный подход к решению задачи классификации картинок видится следующим:

- Обучить несколько разновидностей CNN (например предобученных Densenet, ResNet и VGG)
- Обучить ансамбль из этих CNN

Причем, это важно, обучение каждой CNN нужно проводить при помощи random search по гиперпараметрам. В процессе обученя нужно сохранять веса моделей при различных наборах гиперпараметров, но в итоге брать веса, которые дали наименьший loss на валидационном датасете.

-----------------------------------------------------------------------------

Я обучал модели в Gogole Colab. Google Colab не позволяет запускать долгую сессию. Например, для реализации random search по 10-ти наборам гиперпараметров, с использованием 20-ти эпох на обучение по одному набору гиперпараметров, уйдет 10 х 20 х 20 мин = 4000 мин. Google Colab обычно обрывает сессию через 4-8 часов (всегда по-разному).

-----------------------------------------------------------------------------

Из-за ограничения сессии по времени в полной мере реализовать правильный подход не получилось, хотя я и создал для него ноутбук TinyImageNet_randomsearch.ipynb.

В этом ноутбуке проведено демонстрационное обучение моделей Densenet и ResNet на нескольких эпохах с random search по нескольким наборам гиперпараметров.

На высокий результат рассчитывать при этом не приходится, но данный ноутбук предлагается как демонстрация правильного подхода к решению задачи. При необходимости можно задать сколь угодно много наборов гиперпараметров и много эпох, по которым будет проходить обучение и выбираться наилучшая модель. Но нужна устойчивая вычислительная среда, в которой вычисления будут доведены до конца.

В ноутбуке TinyImageNet_randomsearch.ipynb реализованы следующие этапы:

1. Реализован функционал генерации наборов гиперпараметров для random search

2. Демонстрационно (по 3 эпохи, по 3 набора гиперпараметров) обучены модели Densenet161 и ResNet152.
Best Densenet161 val accuracy = 66.2%
Best ResNet152 val accuracy = 61.8%

3. Демонстрационно (5 эпох) обучен ансамбль из двух моделей.
Best EnsembleModel val accuracy = 63.9%. Думаю, результат можно значительно улучшить, если увеличить кол-во эпох до 15-20.

-----------------------------------------------------------------------------

Результат получше был получен при обучении предобученных моделей без random search, но на большем количстве эпох. В идеале нужно 30-40 эпох. Google Colab позволяет провести 10-20 эпох без разрыва сессии.

В ноутбуке TinyImageNet_no_randomsearch.ipynb реализованы следующие этапы:

1.a. Обучена модель Densenet161 на 20 эпохах.
Val accuracy = 68.8%
Веса этой модели использовалась для итоговой классификации тестового датасета.

1.b. Обучена модель ResNet152 на 10 эпохах.
Val accuracy = 66.9%

1.c. Обучена модель VGG19bn на 10 эпохах.
Val accuracy = 41%

2. Написан функционал для обучения ансамбля переобученных моделей, но обучение провести не удалось, т.к. одна эпоха занимает много времени (в основном из-за VGG), и Google Colab разрывает сессию после нескольких эпох.